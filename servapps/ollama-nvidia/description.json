{
  "name": "Ollama - NVIDIA",
  "description": "LLMs inference server with OpenAI compatible API",
  "longDescription": "<p>Get up and running with Llama 3, Mistral, Gemma, and other large language models.</p>",
  "tags": ["self-hosted", "BigBearCasaOS"],
  "repository": "",
  "image": "https://hub.docker.com/r/ollama/ollama",
  "supported_architectures": ["amd64", "arm64"]
}
